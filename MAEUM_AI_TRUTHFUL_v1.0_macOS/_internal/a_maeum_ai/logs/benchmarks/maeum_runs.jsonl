{"config": {"suite_name": "top8", "benchmark_name": "simple_latency", "task_name": "latency_throughput", "concurrency": 4, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 256, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "94fa5111-d629-45b6-b7e0-2c176dbc3035"}, "start_time": "2025-12-02T04:18:09.705514", "end_time": "2025-12-02T04:18:24.847504", "duration_s": 15.14199, "latency_stats": {"avg_ms": 12047.152966726571, "p50_ms": 15075.930000049993, "p90_ms": 15075.966080045328, "p99_ms": 15075.974198044278, "min_ms": 5989.5538000855595, "max_ms": 15075.975100044161}, "throughput_qps": 0.19898528779035796, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 5.136764705882348, "cpu_mem_peak_mb": 13841.5078125}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 3, "avg_latency_ms": 9885.991766660785, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "latency_throughput_only", "description": "Simple latency/throughput benchmark for Maeum API."}}
{"config": {"suite_name": "top8", "benchmark_name": "perplexity_long", "task_name": "long_context", "concurrency": 1, "num_requests": 8, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "06f2ba8a-e20a-44c7-a15f-3e1697490b9f"}, "start_time": "2025-12-02T04:18:25.358516", "end_time": "2025-12-02T04:18:55.297567", "duration_s": 29.939051, "latency_stats": {"avg_ms": 4976.358016720042, "p50_ms": 3058.905850048177, "p90_ms": 9074.559950036928, "p99_ms": 9078.128765127622, "min_ms": 2758.7463001254946, "max_ms": 9078.525300137699}, "throughput_qps": 0.20094616977593632, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 2.6323308270676677, "cpu_mem_peak_mb": 13765.015625}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {"note": "Perplexity-like long context benchmark using generation quality proxy."}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 9, "avg_latency_ms": 6611.230044470479, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "long_context_quality", "description": "Long-context generation latency and power benchmark."}}
{"config": {"suite_name": "top8", "benchmark_name": "coqa_simple", "task_name": "coqa", "concurrency": 1, "num_requests": 8, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate", "mode": "balanced", "max_tokens": 256, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "def87c1b-718b-4ad1-ae47-08a3ad328c47"}, "start_time": "2025-12-02T04:18:55.807117", "end_time": "2025-12-02T04:19:19.243705", "duration_s": 23.436588, "latency_stats": {"avg_ms": 5834.958324965555, "p50_ms": 5884.650649968535, "p90_ms": 9089.757589949295, "p99_ms": 9099.994828980416, "min_ms": 2469.3996999412775, "max_ms": 9101.132299983874}, "throughput_qps": 0.17137584275474374, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 1.834299516908213, "cpu_mem_peak_mb": 13743.07421875}, "accuracy_metrics": {"accuracy": 1.0, "f1": 0.08712121212121213, "em": 1.0, "score": null, "extra": {"description": "Simple CoQA-style F1/EM over small Korean/English samples.", "num_turns": 4}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 13, "avg_latency_ms": 6371.362353836258, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "coqa_simple", "description": "Small CoQA-style conversational QA benchmark."}}
{"config": {"suite_name": "top8", "benchmark_name": "klue_mini", "task_name": "klue_nli", "concurrency": 1, "num_requests": 4, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate", "mode": "balanced", "max_tokens": 256, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "2910123e-a067-4568-a7fb-f41c9034234f"}, "start_time": "2025-12-02T04:19:19.762777", "end_time": "2025-12-02T04:19:42.688797", "duration_s": 22.92602, "latency_stats": {"avg_ms": 5702.510724950116, "p50_ms": 5704.170749988407, "p90_ms": 9082.451839884743, "p99_ms": 9082.984333853237, "min_ms": 2318.657899973914, "max_ms": 9083.043499849737}, "throughput_qps": 0.17535986298926282, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 1.8009900990099006, "cpu_mem_peak_mb": 13725.20703125}, "accuracy_metrics": {"accuracy": 0.5, "f1": null, "em": null, "score": null, "extra": {"description": "Mini KLUE-style NLI accuracy over small Korean samples.", "num_samples": 4}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 17, "avg_latency_ms": 6213.199217639425, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "klue_mini_nli", "description": "Mini KLUE-style NLI benchmark."}}
{"config": {"suite_name": "mlperf_tiny", "benchmark_name": "mlperf_tiny", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "24fd09ef-366d-45b6-bbcb-0f940c66e98e"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "mlperf_tiny adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "aix", "benchmark_name": "aix", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "6c0a1fc7-b40a-4e6d-8989-5f047e1cb109"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "aix adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "korean_core", "benchmark_name": "kobest", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "7e0b6c32-697e-406a-a2cb-e10e45b7a1fb"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "kobest adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "korean_core", "benchmark_name": "klue_full", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "5643cfe0-993d-4a3b-8982-f53b939f94b8"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "klue_full adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "korean_core", "benchmark_name": "kmmlu", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "e1eb67b4-07be-4f2d-aed4-22f73c66401b"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "kmmlu adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "safety", "benchmark_name": "k_safety", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "51e597d2-1bfe-4664-98ae-e2412ce1872f"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "k_safety adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "emotion", "benchmark_name": "k_emotion", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "5f21786c-0a51-4fe0-8ddb-fdfc684c181d"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "k_emotion adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "leaderboard", "benchmark_name": "ko_llm_leaderboard", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "58ebbccb-7b58-46a8-bf6e-4e306d6ef517"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "ko_llm_leaderboard adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "multilingual", "benchmark_name": "xtreme", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "50fdcfc3-3d9c-4bbd-acfb-7471a7ef111b"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "xtreme adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "multilingual", "benchmark_name": "xnli", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "9d8d2d45-dc88-4852-9c69-1033ce993456"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "xnli adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "dialog_ux", "benchmark_name": "dstc", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "d7effd98-6fc7-4b84-8f6e-b580aad56ae7"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "dstc adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "robustness", "benchmark_name": "robustness", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "adaa739d-0c36-4586-bd6c-baa0c045ad77"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "robustness adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "b2g", "benchmark_name": "custom_b2g", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "879eaa98-c695-41d8-a86e-01e20100731b"}, "start_time": "2025-12-02T04:19:43.207213", "end_time": "2025-12-02T04:19:43.207213", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "custom_b2g adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "top8", "benchmark_name": "simple_latency", "task_name": "latency_throughput", "concurrency": 4, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 256, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "7f3c7dc4-1295-4ad5-91b5-b3dc82ff419c"}, "start_time": "2025-12-02T04:23:31.660202", "end_time": "2025-12-02T04:23:46.758241", "duration_s": 15.098039, "latency_stats": {"avg_ms": 11976.288233340407, "p50_ms": 15017.870200099424, "p90_ms": 15017.891399981454, "p99_ms": 15017.89616995491, "min_ms": 5893.097799969837, "max_ms": 15017.896699951962}, "throughput_qps": 0.1997528364936298, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 4.779999999999997, "cpu_mem_peak_mb": 13883.25390625}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 3, "avg_latency_ms": 11730.459599988535, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "latency_throughput_only", "description": "Simple latency/throughput benchmark for Maeum API."}}
{"config": {"suite_name": "top8", "benchmark_name": "perplexity_long", "task_name": "long_context", "concurrency": 1, "num_requests": 8, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "63e19805-bd16-4b73-9e1d-fae34a524ff5"}, "start_time": "2025-12-02T04:23:47.267342", "end_time": "2025-12-02T04:24:17.024434", "duration_s": 29.757092, "latency_stats": {"avg_ms": 4943.672633341824, "p50_ms": 3065.0294499937445, "p90_ms": 9084.819750045426, "p99_ms": 9100.332105031703, "min_ms": 2657.825900008902, "max_ms": 9102.055700030178}, "throughput_qps": 0.202274411918811, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 2.5375000000000005, "cpu_mem_peak_mb": 13844.67578125}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {"note": "Perplexity-like long context benchmark using generation quality proxy."}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 9, "avg_latency_ms": 7204.157577775833, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "long_context_quality", "description": "Long-context generation latency and power benchmark."}}
{"config": {"suite_name": "top8", "benchmark_name": "coqa_simple", "task_name": "coqa", "concurrency": 1, "num_requests": 8, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate", "mode": "balanced", "max_tokens": 256, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "d9f2053e-f10f-4dbe-9e1c-289769502d19"}, "start_time": "2025-12-02T04:24:17.547053", "end_time": "2025-12-02T04:24:43.741940", "duration_s": 26.194887, "latency_stats": {"avg_ms": 6520.858425006736, "p50_ms": 7033.797850017436, "p90_ms": 9120.677779964171, "p99_ms": 9136.271467951592, "min_ms": 2877.833900041878, "max_ms": 9138.004099950194}, "throughput_qps": 0.15334958665100604, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 2.959656652360512, "cpu_mem_peak_mb": 13828.25}, "accuracy_metrics": {"accuracy": 1.0, "f1": 0.045148247978436654, "em": 1.0, "score": null, "extra": {"description": "Simple CoQA-style F1/EM over small Korean/English samples.", "num_turns": 4}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 13, "avg_latency_ms": 6993.084646152476, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "coqa_simple", "description": "Small CoQA-style conversational QA benchmark."}}
{"config": {"suite_name": "top8", "benchmark_name": "klue_mini", "task_name": "klue_nli", "concurrency": 1, "num_requests": 4, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate", "mode": "balanced", "max_tokens": 256, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "7e6925db-3062-490d-9d01-6ec0c388c290"}, "start_time": "2025-12-02T04:24:44.258814", "end_time": "2025-12-02T04:25:06.901964", "duration_s": 22.64315, "latency_stats": {"avg_ms": 5648.330474970862, "p50_ms": 5650.49969998654, "p90_ms": 9104.184479964897, "p99_ms": 9121.495637954213, "min_ms": 2168.903399957344, "max_ms": 9123.419099953026}, "throughput_qps": 0.1770422968637226, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 2.061691542288557, "cpu_mem_peak_mb": 13785.52734375}, "accuracy_metrics": {"accuracy": 0.5, "f1": null, "em": null, "score": null, "extra": {"description": "Mini KLUE-style NLI accuracy over small Korean samples.", "num_samples": 4}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 17, "avg_latency_ms": 6675.991182352471, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "klue_mini_nli", "description": "Mini KLUE-style NLI benchmark."}}
{"config": {"suite_name": "mlperf_tiny", "benchmark_name": "mlperf_tiny", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "d10d5900-c010-4458-ab21-7896f92f6d0d"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "mlperf_tiny adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "aix", "benchmark_name": "aix", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "ef4d9489-c6e3-473a-96f7-8bb531806c96"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "aix adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "korean_core", "benchmark_name": "kobest", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "8874cb35-49c1-4c5b-8204-633109f7cdb6"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "kobest adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "korean_core", "benchmark_name": "klue_full", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "ed359bae-35b3-4975-97ac-4efd7fb0d625"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "klue_full adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "korean_core", "benchmark_name": "kmmlu", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "855fb148-1123-4b72-b2c9-5694b66af0cf"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "kmmlu adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "safety", "benchmark_name": "k_safety", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "38c67ce6-2efa-4748-9fe8-e27bcbb3d533"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "k_safety adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "emotion", "benchmark_name": "k_emotion", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "716b5fb4-4014-425b-844a-560d5d781a57"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "k_emotion adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "leaderboard", "benchmark_name": "ko_llm_leaderboard", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "1eb6bfc2-bb70-4091-b044-3f337d1a611a"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "ko_llm_leaderboard adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "multilingual", "benchmark_name": "xtreme", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "85830463-7401-4ab1-a85c-802042541856"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "xtreme adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "multilingual", "benchmark_name": "xnli", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "6bb106de-9942-4718-b4c1-27ddeb778cb2"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "xnli adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "dialog_ux", "benchmark_name": "dstc", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "7252e296-2f1b-4816-a537-55d50905ec00"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "dstc adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "robustness", "benchmark_name": "robustness", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "be04c47f-8fac-48c2-af6e-a64632bd184c"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "robustness adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "b2g", "benchmark_name": "custom_b2g", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "8117f714-1f69-4a2b-8633-8b6f0e206aa7"}, "start_time": "2025-12-02T04:25:07.414783", "end_time": "2025-12-02T04:25:07.414783", "duration_s": 0.0, "latency_stats": {"avg_ms": 0.0, "p50_ms": 0.0, "p90_ms": 0.0, "p99_ms": 0.0, "min_ms": 0.0, "max_ms": 0.0}, "throughput_qps": 0.0, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": null, "cpu_mem_peak_mb": null}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {}, "model_info": {}, "server_metrics_snapshot": {}, "notes": {"status": "not_configured", "description": "custom_b2g adapter skeleton. Configure dataset_root to enable."}}
{"config": {"suite_name": "top8", "benchmark_name": "simple_latency", "task_name": "latency_throughput", "concurrency": 4, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 256, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "961c7c11-98fb-466f-b01a-e893a454cc4c"}, "start_time": "2025-12-02T04:27:27.111804", "end_time": "2025-12-02T04:27:42.188251", "duration_s": 15.076447, "latency_stats": {"avg_ms": 12004.380399982134, "p50_ms": 15050.00920011662, "p90_ms": 15050.141520006582, "p99_ms": 15050.171291981824, "min_ms": 5912.957399850711, "max_ms": 15050.174599979073}, "throughput_qps": 0.1993284703575037, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 4.102941176470588, "cpu_mem_peak_mb": 13812.1875}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 20, "avg_latency_ms": 7153.9880450000055, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "latency_throughput_only", "description": "Simple latency/throughput benchmark for Maeum API."}}
{"config": {"suite_name": "top8", "benchmark_name": "perplexity_long", "task_name": "long_context", "concurrency": 1, "num_requests": 8, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "35c8d849-72a6-44ff-adbc-d7b547571f05"}, "start_time": "2025-12-02T04:27:42.714663", "end_time": "2025-12-02T04:28:12.520119", "duration_s": 29.805456, "latency_stats": {"avg_ms": 4964.1425833494095, "p50_ms": 3000.174400047399, "p90_ms": 9081.826250068843, "p99_ms": 9095.360855082981, "min_ms": 2791.8092999607325, "max_ms": 9096.864700084552}, "throughput_qps": 0.20144079115773164, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 2.6388679245282995, "cpu_mem_peak_mb": 13804.51171875}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {"note": "Perplexity-like long context benchmark using generation quality proxy."}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 26, "avg_latency_ms": 6647.928157669062, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "long_context_quality", "description": "Long-context generation latency and power benchmark."}}
{"config": {"suite_name": "top8", "benchmark_name": "coqa_simple", "task_name": "coqa", "concurrency": 1, "num_requests": 8, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate", "mode": "balanced", "max_tokens": 256, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "e23d1ab1-0c2a-43cf-aa96-67f877584535"}, "start_time": "2025-12-02T04:28:13.038654", "end_time": "2025-12-02T04:28:36.577326", "duration_s": 23.538672, "latency_stats": {"avg_ms": 5855.311474995688, "p50_ms": 5953.546550008468, "p90_ms": 9076.481640059501, "p99_ms": 9088.57661406044, "min_ms": 2424.2322999052703, "max_ms": 9089.920500060543}, "throughput_qps": 0.17078079231967613, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 2.185576923076922, "cpu_mem_peak_mb": 13723.8046875}, "accuracy_metrics": {"accuracy": 1.0, "f1": 0.11742424242424242, "em": 1.0, "score": null, "extra": {"description": "Simple CoQA-style F1/EM over small Korean/English samples.", "num_turns": 4}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 30, "avg_latency_ms": 6541.922449980241, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "coqa_simple", "description": "Small CoQA-style conversational QA benchmark."}}
{"config": {"suite_name": "top8", "benchmark_name": "klue_mini", "task_name": "klue_nli", "concurrency": 1, "num_requests": 4, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate", "mode": "balanced", "max_tokens": 256, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "6ffe3dbd-048a-483e-934c-9bee3d893e1d"}, "start_time": "2025-12-02T04:28:37.102211", "end_time": "2025-12-02T04:28:59.826162", "duration_s": 22.723951, "latency_stats": {"avg_ms": 5664.992900041398, "p50_ms": 5654.792300076224, "p90_ms": 9108.89451005496, "p99_ms": 9121.004091072828, "min_ms": 2228.03739993833, "max_ms": 9122.349600074813}, "throughput_qps": 0.17652138018686492, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 1.9601990049751232, "cpu_mem_peak_mb": 13715.27734375}, "accuracy_metrics": {"accuracy": 0.5, "f1": null, "em": null, "score": null, "extra": {"description": "Mini KLUE-style NLI accuracy over small Korean samples.", "num_samples": 4}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 34, "avg_latency_ms": 6438.37397351094, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"benchmark_type": "klue_mini_nli", "description": "Mini KLUE-style NLI benchmark."}}
{"config": {"suite_name": "mlperf_tiny", "benchmark_name": "mlperf_tiny", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "4680af0c-f2cc-487e-ba70-575dadb8c4e0"}, "start_time": "2025-12-02T04:29:00.344873", "end_time": "2025-12-02T04:29:24.143727", "duration_s": 23.798854, "latency_stats": {"avg_ms": 7910.70913321649, "p50_ms": 9108.105599880219, "p90_ms": 9115.312479808927, "p99_ms": 9116.934027792886, "min_ms": 5506.907599978149, "max_ms": 9117.114199791104}, "throughput_qps": 0.12640651954939622, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 2.7985781990521312, "cpu_mem_peak_mb": 13705.796875}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 37, "avg_latency_ms": 6557.46636214364, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "mlperf_tiny adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "aix", "benchmark_name": "aix", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "397c85b8-0d4b-483e-8462-326dc6fbdd08"}, "start_time": "2025-12-02T04:29:24.664384", "end_time": "2025-12-02T04:29:48.499431", "duration_s": 23.835047, "latency_stats": {"avg_ms": 7923.27946672837, "p50_ms": 9097.325599985197, "p90_ms": 9097.476560110226, "p99_ms": 9097.510526138358, "min_ms": 5574.998500058427, "max_ms": 9097.514300141484}, "throughput_qps": 0.12620726185657416, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 2.954976303317534, "cpu_mem_peak_mb": 13769.4453125}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 40, "avg_latency_ms": 6659.6550699847285, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "aix adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "korean_core", "benchmark_name": "kobest", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "7e8e7e59-77d2-4894-92e7-9c795f45f133"}, "start_time": "2025-12-02T04:29:49.018974", "end_time": "2025-12-02T04:30:12.875506", "duration_s": 23.856532, "latency_stats": {"avg_ms": 7919.047166670983, "p50_ms": 9055.59440003708, "p90_ms": 9073.42232009396, "p99_ms": 9077.433602106757, "min_ms": 5623.66779986769, "max_ms": 9077.87930010818}, "throughput_qps": 0.12627486418867556, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 3.1118483412322253, "cpu_mem_peak_mb": 13727.4453125}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 43, "avg_latency_ms": 6747.30124417151, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "kobest adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "korean_core", "benchmark_name": "klue_full", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "4b8dcce7-b553-4297-a334-3dd0907d7157"}, "start_time": "2025-12-02T04:30:13.399623", "end_time": "2025-12-02T04:30:37.392186", "duration_s": 23.992563, "latency_stats": {"avg_ms": 7981.318666677301, "p50_ms": 9130.403999937698, "p90_ms": 9144.63527998887, "p99_ms": 9147.837318000384, "min_ms": 5665.358900092542, "max_ms": 9148.193100001663}, "throughput_qps": 0.12528970006473492, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 3.0859813084112115, "cpu_mem_peak_mb": 13570.83984375}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 46, "avg_latency_ms": 6827.623819547664, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "klue_full adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "korean_core", "benchmark_name": "kmmlu", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "a784640e-5207-44cb-931b-93ceceae2928"}, "start_time": "2025-12-02T04:30:37.922566", "end_time": "2025-12-02T04:31:01.769106", "duration_s": 23.84654, "latency_stats": {"avg_ms": 7937.669333303347, "p50_ms": 9116.863500094041, "p90_ms": 9145.144220022485, "p99_ms": 9151.507382006384, "min_ms": 5543.930099811405, "max_ms": 9152.214400004596}, "throughput_qps": 0.12597879645748436, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 4.643457943925234, "cpu_mem_peak_mb": 13574.03515625}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 49, "avg_latency_ms": 6895.418448967631, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "kmmlu adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "safety", "benchmark_name": "k_safety", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "ef3e645e-94ad-40ef-974d-42362ed5e891"}, "start_time": "2025-12-02T04:31:02.277704", "end_time": "2025-12-02T04:31:26.295742", "duration_s": 24.018038, "latency_stats": {"avg_ms": 7990.164033292483, "p50_ms": 9103.305700002238, "p90_ms": 9124.522339878604, "p99_ms": 9129.296083850786, "min_ms": 5737.3599000275135, "max_ms": 9129.826499847695}, "throughput_qps": 0.1251518535227641, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 6.537962962962963, "cpu_mem_peak_mb": 13592.76953125}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 52, "avg_latency_ms": 6958.420599994357, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "k_safety adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "emotion", "benchmark_name": "k_emotion", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "ddcb0181-bf4d-4155-b1ef-21bd48aefa9b"}, "start_time": "2025-12-02T04:31:26.814542", "end_time": "2025-12-02T04:31:50.658019", "duration_s": 23.843477, "latency_stats": {"avg_ms": 7914.46530004032, "p50_ms": 9040.36429990083, "p90_ms": 9055.888620018959, "p99_ms": 9059.381592045538, "min_ms": 5643.2619001716375, "max_ms": 9059.769700048491}, "throughput_qps": 0.12634838522895267, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 2.7563981042654, "cpu_mem_peak_mb": 13593.09765625}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 55, "avg_latency_ms": 7010.4036563626405, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "k_emotion adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "leaderboard", "benchmark_name": "ko_llm_leaderboard", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "f5252bed-1182-4304-b26c-cf7f7ae15063"}, "start_time": "2025-12-02T04:31:51.176955", "end_time": "2025-12-02T04:32:15.003360", "duration_s": 23.826405, "latency_stats": {"avg_ms": 7904.691366789241, "p50_ms": 9076.523800147697, "p90_ms": 9095.529720140621, "p99_ms": 9099.806052139029, "min_ms": 5537.269100081176, "max_ms": 9100.281200138852}, "throughput_qps": 0.1265037426090244, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 2.873933649289099, "cpu_mem_peak_mb": 13628.890625}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 58, "avg_latency_ms": 7056.475589658005, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "ko_llm_leaderboard adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "multilingual", "benchmark_name": "xtreme", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "c96def72-e91a-46f3-a873-627f7a3f06a3"}, "start_time": "2025-12-02T04:32:15.524862", "end_time": "2025-12-02T04:32:39.337820", "duration_s": 23.812958, "latency_stats": {"avg_ms": 7900.49326668183, "p50_ms": 9063.121400075033, "p90_ms": 9070.318679930642, "p99_ms": 9071.938067898154, "min_ms": 5566.2404000759125, "max_ms": 9072.117999894544}, "throughput_qps": 0.12656949763191083, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 2.9289099526066313, "cpu_mem_peak_mb": 13643.625}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 61, "avg_latency_ms": 7097.832963931695, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "xtreme adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "multilingual", "benchmark_name": "xnli", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "9c9bb55a-643a-4052-b266-20b229430d89"}, "start_time": "2025-12-02T04:32:39.856115", "end_time": "2025-12-02T04:33:03.751547", "duration_s": 23.895432, "latency_stats": {"avg_ms": 7932.984666588406, "p50_ms": 9091.706400038674, "p90_ms": 9093.190639885142, "p99_ms": 9093.524593850598, "min_ms": 5613.685899879783, "max_ms": 9093.56169984676}, "throughput_qps": 0.12605311283133352, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 3.0009433962264116, "cpu_mem_peak_mb": 13630.34765625}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 64, "avg_latency_ms": 7136.858549998578, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "xnli adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "dialog_ux", "benchmark_name": "dstc", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "7d2144d0-0892-4d62-9932-6bd0c4e44c4e"}, "start_time": "2025-12-02T04:33:04.263747", "end_time": "2025-12-02T04:33:28.240392", "duration_s": 23.976645, "latency_stats": {"avg_ms": 7957.368233318751, "p50_ms": 9078.808000078425, "p90_ms": 9085.352319991216, "p99_ms": 9086.824791971594, "min_ms": 5706.3082999084145, "max_ms": 9086.988399969414}, "throughput_qps": 0.12566756811426294, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 4.958744394618832, "cpu_mem_peak_mb": 13660.01171875}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 67, "avg_latency_ms": 7173.4971492516515, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "dstc adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "robustness", "benchmark_name": "robustness", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "375ceb26-b2cd-4e33-b5e3-dd8c7846f683"}, "start_time": "2025-12-02T04:33:28.753864", "end_time": "2025-12-02T04:33:52.589544", "duration_s": 23.83568, "latency_stats": {"avg_ms": 7923.319233348593, "p50_ms": 9074.143399950117, "p90_ms": 9116.873560007662, "p99_ms": 9126.48784602061, "min_ms": 5568.258200073615, "max_ms": 9127.556100022048}, "throughput_qps": 0.126206501020426, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 5.493243243243242, "cpu_mem_peak_mb": 13674.17578125}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 70, "avg_latency_ms": 7205.533789996324, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "robustness adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
{"config": {"suite_name": "b2g", "benchmark_name": "custom_b2g", "task_name": "official", "concurrency": 1, "num_requests": 50, "duration_s": null, "host": "localhost", "port": 6666, "endpoint": "/generate/simple", "mode": "balanced", "max_tokens": 512, "temperature": 0.7, "model_name": null, "model_variant": null, "quantization": null, "compression_techniques": null, "context_window": null, "run_id": "20d3224d-7994-46f6-9aa6-ffae7b700595"}, "start_time": "2025-12-02T04:33:53.099326", "end_time": "2025-12-02T04:34:16.980951", "duration_s": 23.881625, "latency_stats": {"avg_ms": 7936.992266681045, "p50_ms": 9105.183399980888, "p90_ms": 9126.70764005743, "p99_ms": 9131.550594074652, "min_ms": 5573.704699985683, "max_ms": 9132.088700076565}, "throughput_qps": 0.1259899756836459, "power_stats": {"power_avg_w": null, "power_max_w": null, "gpu_util_avg": null, "gpu_mem_peak_mb": null, "gpu_temp_max_c": null, "cpu_util_avg": 3.943577981651374, "cpu_mem_peak_mb": 13626.57421875}, "accuracy_metrics": {"accuracy": null, "f1": null, "em": null, "score": null, "extra": {}}, "system_info": {"os": {"system": "Windows", "release": "10", "version": "10.0.26100"}, "python": "3.10.10", "cpu": {"name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cores": 8, "threads": 16}, "memory": {"total_gb": 31.15}}, "model_info": {"model_name": "MAEUM-3-4B-IT", "version": "5.0.0"}, "server_metrics_snapshot": {"metrics": {"total_queries": 73, "avg_latency_ms": 7235.496715068409, "cache_hits": 0, "customer_satisfaction": 0, "hardware_mode": "gpu_vulkan", "gpu_available": true, "multimodal_queries": 0, "learning_applied": 0, "dotname_rag_used": 0}, "hardware": {"cpu_name": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD", "cpu_count": 8, "total_ram_gb": 31.150943756103516, "gpu_available": true, "gpu_name": "NVIDIA GeForce RTX 5080", "gpu_memory_gb": 3.9990234375, "hardware_mode": "gpu_vulkan"}, "model": {"model_name": "MAEUM3-VL-4B-Instruct", "version": "5.0.0", "multimodal": true, "learning_enabled": false}}, "notes": {"status": "synthetic_mini", "description": "custom_b2g adapter running small synthetic workload against Maeum API. Official dataset integration is pending.", "dataset_root": null}}
