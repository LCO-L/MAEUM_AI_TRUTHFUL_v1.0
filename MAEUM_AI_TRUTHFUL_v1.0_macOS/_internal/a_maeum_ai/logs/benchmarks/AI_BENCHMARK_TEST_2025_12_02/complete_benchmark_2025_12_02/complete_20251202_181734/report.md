# Maeum AI Complete Benchmark Report

**Generated:** 20251202_181734

**Total Benchmarks:** 1

**Modes:** standard

## System Information

- **OS:** Windows 10
- **Python:** 3.10.10
- **CPU:** AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD (8 cores)
- **Memory:** 31.15 GB
- **GPU:** NVIDIA GeForce RTX 5080 (16303.0 MB)

## Summary Table

| Benchmark | Mode | Latency (avg) | P99 | Accuracy | Power | QPS/W |
|-----------|------|---------------|-----|----------|-------|-------|
| CoQA | standard | 8463.4ms | 22447.9ms | 1.000 | 204.4W | 0.0006 |

## Category Summary


### CONVERSATION

- Average Latency: 8463.4ms
- Average Accuracy: 1.0000
- Benchmarks: 1

## Mode Comparison


### STANDARD Mode
- Average Latency: 8463.4ms
- Benchmarks: 1

## Comparison with GPT-4 / Claude / Gemini

| Benchmark | Maeum | GPT-4 | GPT-4o | Claude 3.5 | Gemini 1.5 | Llama 3.1 | Qwen 2.5 |
|-----------|-------|-------|--------|------------|------------|-----------|----------|
| MMLU | - | 86.4 | 88.7 | 88.7 | 85.9 | 86.0 | 86.1 |
| MMLU-Pro | - | 63.7 | 72.6 | 78.0 | - | 66.4 | 71.1 |
| HellaSwag | - | 95.3 | 96.5 | 96.0 | 92.5 | 88.0 | 88.5 |
| ARC-Challenge | - | 96.3 | 97.4 | 96.7 | 94.2 | 93.0 | 93.2 |
| WinoGrande | - | 87.5 | 89.2 | - | - | 85.3 | - |
| TruthfulQA | - | 59.0 | 62.0 | - | - | 54.0 | - |
| GSM8K | - | 92.0 | 94.5 | 96.4 | 91.0 | 95.1 | 95.8 |
| MATH | - | 42.5 | 52.6 | 71.1 | 58.5 | 68.0 | 83.1 |
| HumanEval | - | 67.0 | 90.2 | 92.0 | 71.9 | 80.5 | 86.6 |
| MBPP | - | 72.0 | 85.0 | 90.0 | - | - | 88.2 |
| BBH | - | 83.1 | 87.3 | 89.0 | 84.0 | 81.6 | 88.3 |
| CoQA | 100.0 | 89.0 | - | - | - | - | - |

*Note: Reference scores from official publications (OpenAI, Anthropic, Google, Meta, Alibaba)*